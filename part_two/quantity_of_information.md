## 情報の量
### 5.1 情報とは

```
本書では, 自然の状態についての記述を情報と呼び, 情報を発するものを情報源と呼ぶことにする
例えば, 天気予報で「東京都では今日の午後から雨になるでしょう」という情報が発せられた場合は, 天気予報が情報源である
```

### 5.2 情報量のイメージ

```
情報量は, 不確実性の減少量と定義できる
「情報量がない発言」の例をいくつか挙げる

  - 明日の天気は, 雨が降る, もしくは, 雨が降らないかのどちらかになる
  - サイコロの目は1, 2, 3, 4, 5, 6のどれかの数字になった

情報量がない発言は「当たり前」の内容を発言しており, その発言に情報量はない
```

### 5.3 自己情報量
#### 5.3.1 情報量と確率

```
発生する確率が低いことを知らされると情報量が多いとされる
サイコロの目において, 1, 2, 3, 4, 5, 6のどれからが出てくる確率は1であるので, 先述の発言に情報量はない
一方で「偶数の目が出た」といった記述には情報量がある
サイコロで偶数の目が出る確率は0.5であり, やや珍しいことが起こったわけである
```

```
上記の議論をまとめると, 情報量の持つ性質として, 確率の減少関数であることが想定される
これを数式で表現する. ただし, 事象xが起こったことを知らされたときの情報量をi(x)とする
また事象xが起こる確率をP(x)とする
```

<img width="486" alt="スクリーンショット 2021-07-18 13 21 59" src="https://user-images.githubusercontent.com/43327056/126055615-f6e244a0-1f24-4d68-99e3-f805395e98cb.png">

```
このときi(P(x))はP(x)の減少関数となる
すなわちP(x)が増加するとい(P(x))が小さくなる（起こる確率が高いものほど, それを教えてもらっても情報量が少ない）
```

#### 5.3.2 情報量の加法性

```
続いて情報量の加法性に注目する
情報をまとめて教えられた場合と, 独立な情報を小出しにして教えられた場合で, 情報の総量は変わらないはずである
独立という用語は後ほど定義を解説するが, 平たく言えば「関係のない事象」くらいの意味である
例えば「偶数の目が出る」と「3の倍数の目が出る」は互いに独立である
一方で「偶数の目が出る」と「6の目が出る」は独立ではない（6の目は確実に偶数なので）
```

```
例えば「サイコロで6の目が出た」と教えてもらったとする
このときの情報量は「偶数の目が出た」という発言の情報量と「3の倍数の目が出た」という発言の情報量の合計値になるはずである
（偶数かつ3の倍数の目は6しかないので, この2つの情報で目を1つに絞ることができる）

数式で表現すると以下のようになる
```

<img width="486" alt="スクリーンショット 2021-07-18 13 37 31" src="https://user-images.githubusercontent.com/43327056/126055905-afdc65af-73f5-456e-a8ec-4e6a77cf4299.png">

```
ここで, 情報量が事象の確率の関数であることを考えると, 下記のようになる
ただし6の目が出る確率は1/6であり, 偶数の目が出る確率は1/2であり, 3の倍数の目が出る確率は1/3である
```

<img width="486" alt="スクリーンショット 2021-07-18 13 39 16" src="https://user-images.githubusercontent.com/43327056/126055949-b1daef90-36fd-4d97-88e2-30944221ae5d.png">

```
一般的には以下のようになる. ただしxとyは互いに独立な事象である
```

<img width="486" alt="スクリーンショット 2021-07-18 13 42 29" src="https://user-images.githubusercontent.com/43327056/126056011-94a75f09-b770-4f4f-82d1-fe0bf3dec23d.png">

#### 5.3.3 自己情報量の定義

```
情報量が満たしていてほしい性質を整理する
  - 情報量は関数の減少関数である
  - 情報量は加法性を満たす

上記2つの性質を持ち, それでいて微分可能であるという条件をつけると,
「確率P(x)で現れる事象xが発生したことを教えてもらったとき」の自己情報量は次のように定義できる
```

<img width="486" alt="スクリーンショット 2021-07-18 13 46 54" src="https://user-images.githubusercontent.com/43327056/126056094-4e746c57-e87c-4ddd-8636-98e757741dfd.png">

```
ただしbは対数の底である. 対数の底が2であるときの情報量の単位をbitと呼ぶ

サイコロの例を使って自己情報量を計算する
「6の目が出た」とわかったときの情報量, 「偶数の目が出た」とわかったときの情報量は,
-log_2(1/6)≈2.58, -log_2(1/2)=1である
「3の倍数の目が出た」とわかったときの情報量は, -log_2(1/3)≈1.58である
-log_2(1/6) = -log_2(1/2)-log_2(1/3)であることから, 加法性を満たしていることがわかる
```

#### 5.3.4 Python実装

```python
def self_info(prob):
    return -1 * np.log2(prob)

i_6 = self_info(1/6)
i_3mul = self_info(1/3)
i_even = self_info(1/2)

print(f"6の目が出たとわかったときの自己情報量: {i_6: .3g}")
print(f"3の倍数とわかったときの自己情報量: {i_3mul: .3g}")
print(f"偶数であるとわかったときの自己情報量: {i_even: .3g}")
print(f"情報を小出しにされたときの合計値: {i_3mul + i_even: .3g}")
```

### 5.5 平均情報量と情報エントロピー
#### 5.5.1 平均情報量の定義

```
平均情報量を紹介する
これは, 下記のように自己情報量の期待値をとったものである
ただしI(θ)は自然の状態の平均情報量である. i(θ_i)は自然の状態がθ_iだとわかったときの自己情報量である
P(θ_i)は自然の状態がθ_iである確率である
```

<img width="486" alt="スクリーンショット 2021-07-18 15 43 30" src="https://user-images.githubusercontent.com/43327056/126058271-3fa1047d-eba7-4b7c-a7e3-9c4c1a9eae48.png">

```
自己情報量の定義を代入する. 対数の底は2とする
なお, 確率P(θ_i) = 0のときは, 0・log_20 = 0とみなす
```

<img width="486" alt="スクリーンショット 2021-07-18 15 51 30" src="https://user-images.githubusercontent.com/43327056/126058428-e7f02eaa-0b38-44a7-bdd3-d053e18dcbc0.png">

#### 5.5.2 数値例

```
数値例を挙げる. 自然の状態を景気の動向とする. θ_iが好況で, θ_2が不況である
仮に次のような確率分布になっていたとする
```

<img width="486" alt="スクリーンショット 2021-07-18 15 54 46" src="https://user-images.githubusercontent.com/43327056/126058483-249c3930-d84f-4feb-9fd8-c3049df69730.png">

```
まだ景気がどうなるかわかっておらず, 景気の動向が明日発表されるものとする
この結果を確認することで得られるであろう情報量の期待値は次のように計算される
```

<img width="486" alt="スクリーンショット 2021-07-18 15 57 01" src="https://user-images.githubusercontent.com/43327056/126058531-d85229b6-9e56-4a90-916f-bb2be072bb75.png">

#### 5.5.3 情報エントロピー

```
平均情報量はエントロピーとも呼ばれる. 本書では, 情報エントロピーと呼ぶことにする
情報エントロピーとは不確実性の大きさと解釈される

自然の状態に対する情報エントロピーをH(θ)と表記することにする
情報エントロピーは平均情報量と定義が全く同じである. すなわち, I(θ) = H(θ)である
```

```
平均情報量と, 不確実性の大きさ（情報エントロピー）の定義が同じという点について補足する
最も直感的な解釈は「情報量とは, 不確実性の減少量である」という定義に戻ることである

不確実性が最初H(θ)あったとする. 自然の状態が明らかになると, この不確実性が0になる
不確実性がH(θ)から0になったわけなので, 「不確実性の減少量 = 情報量」はH(θ)そのものである

もう1つの解釈は「不確実性が大きい状況下でもらえる情報は嬉しい（情報量が多い）」というものである
```

#### 5.5.4 Python実装

```python
from scipy.stats import entropy

prob_state = pd.Series([0.4, 0.6])
prob_state.index = ["好況", "不況"]

H_stats = entropy(prob_state, base=2)

print(f"{H_stats:.3g}")
```

### 5.6 情報エントロピーの性質

```
様々な確率分布に対して情報エントロピーを計算して, その特徴を見ていく
{P(θ_1) = 0, P(θ_2) = 1}という不確実性がない（確実にθ_2が出る）分布から始めて,
{P(θ_1) = 0.1, P(θ_2) = 0.9} そして {P(θ_1) = 0.2, P(θ_2) = 0.8} ... と
0.1ずつP(θ_1)を増やしていきながら, 情報エントロピーを計算する
```

```python
prob_df = pd.DataFrame({
    "p1": np.arange(start=0, stop=1.1, step=0.1),
    "p2": 1 - np.arange(start=0, stop=1.1, step=0.1)
})
prob_df["entropy"] = prob_df[["p1", "p2"]].apply(entropy, axis=1, base=2)
prob_df.plot(x="p1", y="entropy")
```

<img width="486" alt="スクリーンショット 2021-07-18 16 37 36" src="https://user-images.githubusercontent.com/43327056/126059590-5a1717f0-782b-482c-af32-4f20780637dd.png">

```
この図を見ると, P(θ_1)が0.5に近い（すなわちP(θ_2)も0.5に近い）ときに, 情報エントロピーが大きくなっている
θ_1になるかθ_2になるのか予想できないとき, 不確実性が大きくなるわけである
逆にP(θ_1)が1あるいは0に近いときは不確実性は小さくなる
```

### 5.7 情報としての予測

```
ここからは自然の状態と, 自然の状態について何かを教えてくれる情報の関係性をより詳細に見ていく
本章では需要予測を例に挙げる. 以下では情報のことを予測値と記す
実際の自然の状態は, 区別するために実測値と表記する
予測値の集合はFと表記する. 個別の予測値はf_kであり, f_k∈Fである
集合Fは有限情報として, その要素数の個数を#Fとする
```

### 5.8 同時分布・周辺分布・条件付き分布

```
予測の持つ情報量を計算する前に, 確率の表記法を整理する
まずは確率の形式的な表記を解説して, その後数値例を見る
```

#### 5.8.1 定義

```
次のような, 実測値と予測値の分割表があるとする
θ_1が好況, θ_2が不況, f_1が好況予測, f_2が不況予測とする
```

<img width="486" alt="スクリーンショット 2021-07-18 17 25 15" src="https://user-images.githubusercontent.com/43327056/126060792-4ac94383-a412-4384-9bd8-50ac7b645d09.png">

```
ここでP(θ_i, f_k)は自然の状態がθ_iであり, かつ, 予測結果がf_kである同時確率である
同時確率の分布を同時分布と呼ぶ
予測値と実測値が一致する確率はP(θ_1, f_1) + P(θ_2, f_2)で計算される
これが予測の的中率である
```

```
P(θ_1)は自然の状態がθ_1である確率である. これは同時確率の和として計算できる
例えばP(θ_1) = P(θ_1, f_i) + P(θ_1, f_2)である
同様にP(θ_2) = P(θ_2, f_i) + P(θ_2, f_2)と計算される
```

```
一般には下記のようになる
同時確率における「相手」を変更しながら合計値を計算するイメージである
この計算を周辺化と呼ぶ
```

<img width="486" alt="スクリーンショット 2021-07-19 11 19 40" src="https://user-images.githubusercontent.com/43327056/126093600-246a1b38-0134-4670-afd5-05914021e801.png">

```
同様に, P(f_k)は次のように計算される
```

<img width="486" alt="スクリーンショット 2021-07-19 11 20 01" src="https://user-images.githubusercontent.com/43327056/126093608-18d446ec-bf73-499a-9d73-10684f8b3dbd.png">

```
θ_iやf_kの分布は周辺分布と呼ぶ
```

```
予測f_1が出たという条件で, 実際に自然の状態がθ_1になる確率をP(θ_1|f_1)と表記する
P(θ_1|f_1) = P(θ_1|f_1) ÷P(f_1)と計算される. これを条件付き確率と呼ぶ

条件付き確率の分布を条件付き分布と呼ぶ. 一般には以下のようになる
```

<img width="486" alt="スクリーンショット 2021-07-19 13 50 15" src="https://user-images.githubusercontent.com/43327056/126104664-7ceb2734-d836-4ac6-bc2d-f93adcd365e0.png">
<img width="486" alt="スクリーンショット 2021-07-19 13 50 34" src="https://user-images.githubusercontent.com/43327056/126104667-e2ce63a8-bb49-4a6f-9ad9-4d9b2e2609d4.png">

```
条件付き確率の定義を式変形すると以下のようになる
```

<img width="486" alt="スクリーンショット 2021-07-19 13 56 25" src="https://user-images.githubusercontent.com/43327056/126105123-31e38f60-4d54-4e5e-a17f-eb26cb93cef2.png">

```
同時確率は, 「条件付き確率x条件が発生する確率」で計算できるわけである
すべてのi, kにおいてP(θ_i, f_k) = P(θ_i)P(f_k)と計算されるとき, θとfが独立であるという
独立であるときにはP(θ_i, f_k) = P(θ_i)になる
「予測があってもなくても, 実測値の分布が全く変わらない」ときに予測値と実測値が独立になる
```

#### 5.8.2 数値例

```
数値例を見ていく. 次のような同時分布が得られているとする
```

<img width="486" alt="スクリーンショット 2021-07-19 14 26 09" src="https://user-images.githubusercontent.com/43327056/126107481-b0b1dadf-0b84-42de-ac80-c001e7674f13.png">

```
好況と予測して, 実際に好況になる確率P(θ_1, f_1)は0.35である
不況と予測して実際に不況になる確率P(θ_2, f_2)が0.5である
予測の的中率は, 0.35 + 0.5 = 0.85である
```

```
予測を評価する際は, 予測が出されたという条件のもとでの条件付き分布P(θ|f)を参照することが多い
条件付き分布を下記にまとめた. なお, 各数値は小数点以下3桁で丸めている
```

<img width="486" alt="スクリーンショット 2021-07-19 14 36 54" src="https://user-images.githubusercontent.com/43327056/126108176-3fec4fce-d2fe-4fe1-91fc-6331da73b8e6.png">

```
条件付き分布における好況予測f_1の列を見ると「好況と予測されたとき, 実際にはどのような景気になっているか」がわかる
およそ78%が予測通りに好況となっており, 22%は予測が外れて不況となっている
```

```
P(θ_1|f_1) = 0.778とP(θ_1) = 0.4が異なるので, 予測値と実測値は独立ではないことがわかる
```

### 5.9 条件付きエントロピー

```
予測を出した後に, まだ残っている不確実性は, 予測に対する条件付きエントロピーで評価する
```

#### 5.9.1 定義

```
条件付きエントロピーH(θ|f)は以下のように計算される
```

<img width="486" alt="スクリーンショット 2021-07-19 15 29 23" src="https://user-images.githubusercontent.com/43327056/126113056-1c3f42d5-9557-4af4-bbda-ef098d7bd0fb.png">


```
定義通りの計算式はやや解釈が難しいので, 式の変形をする
同時確率P(θ_i|f_k)はP(θ_i|f_k)P(f_k)に分解できる
```

<img width="486" alt="スクリーンショット 2021-07-19 15 30 00" src="https://user-images.githubusercontent.com/43327056/126113153-71fec8ec-878f-4ccf-88c6-7617d0c4a832.png">


```
Σ記号の左右を入れ替えた上で, Σの中身を分解する
```

<img width="486" alt="スクリーンショット 2021-07-19 15 30 17" src="https://user-images.githubusercontent.com/43327056/126113158-93c837bb-638e-44ce-8420-5ea1ba4c9c36.png">

```
条件付き分布P(θ_i|f_k)の情報エントロピーを, P(f_k)で乗じてから和をとることで, 期待値を計算していると解釈できる
```

#### 5.9.2 数値例

```
具体的な数値を入れて確認する. 以下の手順で計算する
  Step1: 予測の内容ごとに, 条件付き分布の情報エントロピーを計算する
  Step2: 上記の結果の期待値をとる
```

```
まずはStep1から進める
k = 1に固定する（好況予測が出たときだけを考える）と, 以下のようになる
これが「好況予測が出た後に残った不確実性」である
```

<img width="486" alt="スクリーンショット 2021-07-19 15 37 17" src="https://user-images.githubusercontent.com/43327056/126114500-e2eeb737-a484-49dd-af97-57f69d2f4edb.png">

```
表2.5.5の数値を入れると以下のようになる
```

<img width="486" alt="スクリーンショット 2021-07-19 15 40 54" src="https://user-images.githubusercontent.com/43327056/126114504-57d0b9cd-5935-4fa5-acd4-248d1ba85261.png">

```
同様に, k = 2に固定する（不況予測が出たときだけを考える）と, 以下のようになる
これが「不況予測が出た後に残った不確実性」である
```

<img width="486" alt="スクリーンショット 2021-07-19 15 41 13" src="https://user-images.githubusercontent.com/43327056/126114508-4d7eb2fa-8827-45b8-bf7c-8be272ea672d.png">

```
表2.5.5の数値を入れると以下のようになる
```

<img width="486" alt="スクリーンショット 2021-07-19 15 41 36" src="https://user-images.githubusercontent.com/43327056/126114509-256f9ecd-5f61-4275-b54b-ed114fe1a2bd.png">

```
不況予測の方が小さな不確実性になった

最後に, 各々の予測（好況予測と不況予測）が出される確率を乗じて, 期待値を計算すれば完成である
P(f_1)とP(f_2)は同時分布の表（表2.5.4）の数値を使用する
```

<img width="486" alt="スクリーンショット 2021-07-19 15 49 29" src="https://user-images.githubusercontent.com/43327056/126115395-900c1e9b-e0bf-4f6f-adbf-3b9de9688d5a.png">

### 5.10 相互情報量
#### 5.10.1 定義

```
自然の状態がもともと持っていた不確実性H(θ)から, 予測が出された後に残っている不確実性H(θ|f)を差し引いたものを
相互情報量と呼ぶ
```

<img width="486" alt="スクリーンショット 2021-07-19 15 54 37" src="https://user-images.githubusercontent.com/43327056/126115988-8c4bfc24-45de-4eca-9a04-010b63afd33a.png">

```
相互情報量は「予測を使うことで減らせた, 自然の状態の不確実性」と解釈できる
不確実性の減少量を情報量と呼ぶなら, 相互情報量は予測が自然の状態に対して持っている情報量だといえる
```

#### 5.10.2 数値例

```
表2.5.4の数値を使うと, 相互情報量は以下のように計算できる
```

<img width="486" alt="スクリーンショット 2021-07-19 15 59 54" src="https://user-images.githubusercontent.com/43327056/126116689-24c4dce2-9593-4ebd-86de-8f3928e64995.png">

### 5.11 Pythonによる相互情報量の計算
#### 5.11.1 同時分布・周辺分布・条件付き分布

```
まずは自然の状態に対する予測値と実測値の同時分布を用意する
```

```python
# 自然の状態に対する予測値と実測値の同時分布
joint_forecast_state = pd.DataFrame({
    "好況予測": [0.35, 0.1],
    "不況予測": [0.05, 0.5]
})
joint_forecast_state.index = ["好況", "不況"]

# 予測値の周辺分布
marginal_forecast = joint_forecast_state.sum(axis=0)

# 実測値の周辺分布
marginal_state = joint_forecast_state.sum(axis=1)

# 条件付き分布P(θ | f)
conditional_forecast = joint_forecast_state.div(marginal_forecast, axis=1)

# 予測結果ごとの不確実性
H_by_f = conditional_forecast.apply(entropy, axis=0, base=2)
H_conditional = H_by_f.mul(marginal_forecast).sum()
print(f"{H_conditional:.3g}")

# 景気に対する, 元々の不確実性
H_state = entropy(marginal_state, base=2)
print(f"{H_state:.3g}")

# 相互情報量
MI = H_state - H_conditional
print(f"{MI:.3g}")
```
